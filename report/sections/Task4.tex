\section{Unsupervised Anomaly Detection and Interpretation}
Finally, we use two totally unsupervised clustering algorithms: K-Means and DB-Scan.

\subsection{K-Means}\label{sec:kmeans}
First, we fit K-Means with 4 clusters using the full training data.

\paragraph{Clusters analysis} \Cref{fig:kmeans_attack_distribution} shows the obtained clusters and their composition. Clusters have different sizes: cluster 2 is the biggest one (10173 samples), followed by cluster 0 (3464 samples), while cluster 1 and 3 have almost the same size (1606 and 1745 samples, respectively). Clusters are not pure, but generally each one has a predominant class. Clusters 0 and 2 are mainly composed by \texttt{normal} samples, while cluster 1 almost contains only \texttt{dos} samples. Cluster 3, instead, is composed by 946 \texttt{probe} samples, 482 \texttt{normal} and 317 \texttt{dos} ones. From this analysis, we infer that K-Means fails to properly separate the different attacks, probably because they have common characteristics.

\paragraph{Silhouette analysis} \Cref{fig:kmeans_silhouette} shows the silhouette scores for the obtained clusters. The points in cluster 1 have the highest silhouette, with an average score of \num{0.735}: \SI{93}{\percent} of its samples is labelled as \texttt{dos}, and these points are well-matched to their own cluster and poorly matched to neighboring clusters. Cluster 2 and 3 have an average silhouette of \num{0.33} and \num{0.25} respectively, indicating that their points are not very cohesive and separated from the ones of the other clusters. In fact, they both contain samples belonging to different attack labels: cluster 2 contains all types of attacks, while in cluster 3 only \texttt{r2l} samples are not present. Finally, cluster 0 has the lowest average silhouette (\num{0.11}), which indicates a very weak clustering: 619 samples (\SI{17.8}{\percent}) have a silhouette lower than zero, implying that they have probably been assigned to the wrong cluster, as a different one is more similar. As mentioned in the previous paragraph, cluster 0 is composed by  \texttt{normal} (2546), \texttt{dos} (628) and \texttt{probe} (290) samples.

\begin{figure}
	\centering
    \subfloat[][{Attack label distribution in clusters.}\label{fig:kmeans_attack_distribution}]
	{\includegraphics[width=.48\linewidth]{img/Task4/task4_kmeans_attack_distribution_rs42.png}} \quad
	\subfloat[][{Clusters silhouette.}\label{fig:kmeans_silhouette}]
	{\includegraphics[width=.48\linewidth]{img/Task4/task4_kmeans_silhouette_rs42.png}} \\
  	\caption{Graphs about K-Means with 4 clusters.}{}
\end{figure}

\paragraph{t-SNE} Then, we used t-SNE algorithm to obtain a 2D visualization of the clustered points, trying three different \emph{perplexity} values (5, 30 and 100). Among them, the representation with \emph{perplexity}$ = 30$ proven to be the most informative one, allowing for understanding class structure and separability quite effectively. In fact, it preserves the local separation without losing the global structure (as happens with higher perplexity) and without over-fragmenting the classes (obtained with lower perplexity).

\Cref{fig:tsne_clusters} shows the t-SNE embedding colored by the K-Means cluster assignments, while \cref{fig:tsne_labels} uses the same embedding but colors points according to the true attack labels. In both cases, we used \emph{perplexity}$ = 30$, as it was previously identified as the best value. This comparison highlights that K-Means essentially partition the map into four contiguous territories, assuming that if points are close together, they must belong to the same category. However, this assumption does not always hold for this dataset.

In particular, several attack types appear in multiple disconnected regions of the embedding: the most misinterpreted points are those close to an area populated by samples of another attack. For example, \texttt{dos} samples (in blue) are distributed across several disconnected ``islands'' in the map, but only those in the bottom-left region are captured by a dedicated cluster (1), while the others are grouped together with \texttt{normal} or \texttt{probe} traffic.. Similarly, \texttt{probe} samples are scattered across multiple regions and are assigned to clusters containing heterogeneous attack labels.  Moreover, the majority of \texttt{normal} flows belong to cluster 2, but the ones on the top and on the bottom-left have been assigned to other clusters along with attack samples. The most misinterpreted points are \texttt{r2l} samples, which are entirely absorbed by cluster 2, dominated by \texttt{normal} traffic, essentially because they have low traffic volume and look statistically similar to normal sessions.

Overall, these visualizations show that while K-Means can successfully identify dense and homogeneous attack patterns such as DoS, it struggles to separate attack types that are sparse, subtle, or overlapping with normal traffic.

\begin{figure}
	\centering
    \subfloat[][{t-SNE using as label the cluster ID.}\label{fig:tsne_clusters}]
	{\includegraphics[width=.48\linewidth]{img/Task4/task4_tsne_kmeans_perplexity_30.png}} \quad
	\subfloat[][{t-SNE using as label the attack label.}\label{fig:tsne_labels}]
	{\includegraphics[width=.48\linewidth]{img/Task4/task4_tsne_true_labels_perplexity_30.png}} \\
  	\caption{t-SNE plots with \emph{perplexity} = 30.}{}
\end{figure}

\subsection{DB-Scan}
Finally, we used DB-Scan, a clustering algorithm designed to detect anomalous patterns that may represent anomalies.

\paragraph{Choosing min\_points and $\epsilon$} To choose the most appropriate value for \emph{min\_points}, we used two different approaches. First, we tried to estimate it running K-Means with different numbers of clusters, and taking the smallest cluster containing only \texttt{normal} points. With 7 clusters, there is a pure cluster formed by 7 \texttt{normal} points. The same results can be obtained also with 8 and 9 clusters. However, using this approach might be somehow misleading, since we are relying on the labels for an unsupervised algorithm. Moreover, this number of \emph{min\_points} is very small, hence the algorithm may tend to create clusters whenever some points are close to each other, even though they are few, anomalous and very far from the others. Hence, we tried a different approach. First, we computed the cosine distance of each point from its first 3000 neighbors. Then, we averaged these distances for all points in the training set, trying both to include all samples and to exclude points with values quite distant from the average, but these choice did not affect the result. We noticed that the average distance of a point from its $3000^{th}$ neighbor is nearly above \num{0.5}, which means that the space is very cohesive, considering that the cosine distance can be at most 1. Hence, we chose as value of \emph{min\_points} 1500: a point is on average distance of \num{0.32} from its $1500^{th}$, and close to this number the distance tends to increase more abruptly, playing the role of an elbow. This value of \emph{min\_points} seemed more reasonable in respect to our goal, since we wish to flag all attack points as anomalous, hence we want to create big clusters of normal points only. As a consequence, we ran DB-Scan using \emph{min\_points}$ = 1500$, $\epsilon = 0.32$, and $metric = cosine$, distance used to emphasize directional similarity in the high-dimensional feature space and reduce the impact of magnitude differences caused by traffic volume.

\begin{table}
\footnotesize
\centering
\caption{Clustering performed by DB-Scan algorithm.}
\label{tab:task4_dbscan_clusters}
\begin{tabular}{ccccc}
\toprule
{Attack Label} & {Noise Cluster} & {Cluster 0} & {Cluster 1} & {Cluster 2} \\
\midrule
Normal 	& 2508 	& 8472 	& 50 	& 1072 	\\
DoS 	& 530 	& 180 	& 1494 	& 418 	\\
Probe 	& 1879 	& 86 	& 78 	& 17 	\\
R2L 	& 64 	& 99 	& 1 	& 0 	\\ \midrule
Total 	& 4981 	& 8836 	& 1623 	& 1507	\\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Clusters analysis} \Cref{tab:task4_dbscan_clusters} summarizes the resulting clustering. DB-Scan identified 4981 anomalies (\SI{29.4}{\percent}), which closely matches the true proportion of anomalous traffic in the dataset.  The noise cluster is composed half by \texttt{normal} samples and half by attack samples, especially \texttt{probe} ones. Only \SI{20}{\percent} of \texttt{dos} points have been flagged as noise, against \SI{40}{\percent} of \texttt{r2l} and \SI{90}{\percent} of \texttt{probe} ones. It means that \texttt{probe} points tend to be sparse and isolated in the feature space, while \texttt{dos} ones form dense and cohesive clusters due to their high traffic volume and repetitive patterns. Cluster 0 is composed for the \SI{95}{\percent} by \texttt{normal} points, while cluster 1 is mainly formed by \texttt{dos} ones. Cluster 2, instead, is composed for \SI{71}{\percent} by \texttt{normal} samples, and for the \SI{27}{\percent} by \texttt{dos} ones, indicating that DB-Scan struggles when anomalous behavior itself becomes dense and overlaps with benign traffic. Overall, we deduce that DB-Scan is effective at finding ``sparse'' anomalies like \texttt{probe} ones, while it struggles when attacks generate dense traffic patterns (like the \texttt{dos} samples assigned to cluster 1 by K-Means in \cref{sec:kmeans}), even if choosing a high value for \emph{min\_points}.

