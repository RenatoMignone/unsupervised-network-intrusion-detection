\section{Unsupervised Anomaly Detection and Interpretation}
In this section, we use two totally unsupervised clustering algorithms: K-Means and DB-Scan. Hence, attack labels are used only for post-hoc evaluation and are not used in the clusters' creation.

\subsection{K-Means}\label{sec:kmeans}
First, we fit K-Means with 4 clusters using the full training data.

\paragraph{Clusters analysis} \Cref{fig:kmeans_attack_distribution} shows the obtained clusters and their composition. Clusters have different sizes: cluster 0 is the biggest one (10647 samples), followed by cluster 2 (4472 samples), while cluster 1 and 3 have almost the same size (1782 and 1930 samples, respectively). Clusters are not pure, but generally each one has a predominant class. Clusters 0 and 2 are mainly composed of \texttt{normal} samples, while cluster 1 almost contains only \texttt{dos} samples. Cluster 3, instead, is composed of 1036 \texttt{probe} samples, 551 \texttt{normal} and 343 \texttt{dos} ones. From this analysis, we infer that K-Means fails to properly separate the different attacks, probably because they have common characteristics.

\paragraph{Silhouette analysis} \Cref{fig:kmeans_silhouette} shows the silhouette scores for the obtained clusters. The points in cluster 1 have the highest silhouette, with an average score of \num{0.737}: \SI{93}{\percent} of its samples is labelled as \texttt{dos}, and these points are well-matched to their own cluster and poorly matched to neighboring clusters. Cluster 0 and 3 have an average silhouette of \num{0.48} and \num{0.25} respectively, indicating that their points are not very cohesive and separated from the ones of the other clusters. In fact, they both contain samples belonging to different attack labels: cluster 0 contains all types of attacks, while in cluster 3 only \texttt{r2l} samples are not present. Finally, cluster 2 has the lowest average silhouette (\num{-0.08}), which indicates a very weak clustering: 3671 samples (\SI{76.4}{\percent}) have a silhouette lower than zero, implying that they have probably been assigned to the wrong cluster, as a different one is more similar. As mentioned in the previous paragraph, cluster 2 is composed of  \texttt{normal} (3339), \texttt{dos} (717), \texttt{probe} (357) and \texttt{r2l} (59) samples.

\begin{figure}
	\centering
    \subfloat[][{Attack label distribution in clusters.}\label{fig:kmeans_attack_distribution}]
	{\includegraphics[width=.48\linewidth]{img/Task4/task4_kmeans_attack_distribution_rs42.png}} \quad
	\subfloat[][{Clusters silhouette.}\label{fig:kmeans_silhouette}]
	{\includegraphics[width=.48\linewidth]{img/Task4/task4_kmeans_silhouette_rs42.png}} \\
  	\caption{Graphs about K-Means with 4 clusters.}{}
\end{figure}

\begin{figure}
	\centering
    \subfloat[][{t-SNE using as label the K-Means cluster ID}\label{fig:tsne_clusters}]
	{\includegraphics[width=.48\linewidth]{img/Task4/task4_tsne_kmeans_perplexity_30.png}} \quad
	\subfloat[][{t-SNE using as label the attack label.}\label{fig:tsne_labels}]
	{\includegraphics[width=.48\linewidth]{img/Task4/task4_tsne_true_labels_perplexity_30.png}} \\
  	\caption{t-SNE plots with \emph{perplexity} = 30.}{}
\end{figure}

\paragraph{t-SNE} Then, we used t-SNE algorithm to obtain a 2D visualization of the clustered points, trying three different \emph{perplexity} values (5, 30 and 100). Among them, the representation with \emph{perplexity} 30 proved to be the most informative one, allowing for understanding class structure and separability quite effectively. In fact, it preserves the local separation without losing the global structure (as happens with higher perplexity) and without over-fragmenting the classes (obtained with lower perplexity).

\Cref{fig:tsne_clusters} shows the t-SNE embedding colored by the K-Means cluster assignments, while \cref{fig:tsne_labels} uses the same embedding but colors points according to the true attack labels. In both cases, we used \emph{perplexity} 30, as it was previously identified as the best value. This comparison highlights that K-Means essentially partition the map into four contiguous territories, assuming that if points are close together, they must belong to the same category. However, this assumption does not always hold for this dataset.

In particular, several attack types appear in multiple disconnected regions of the embedding: the most misinterpreted points are those close to an area populated by samples of another attack. For example, \texttt{dos} samples (in blue) are distributed across several disconnected ``islands'' in the map, but only those in the bottom-left region are captured by a dedicated cluster (1), while the others are grouped together with \texttt{normal} or \texttt{probe} traffic. Similarly, \texttt{probe} samples are scattered across multiple regions and are assigned to clusters containing heterogeneous attack labels.  Moreover, the majority of \texttt{normal} flows belong to cluster 0, but the ones on the top and on the bottom-left have been assigned to other clusters along with attack samples. The most misinterpreted points are \texttt{r2l} samples, which are absorbed by clusters 0 and 2, dominated by \texttt{normal} traffic, essentially because they have low traffic volume and look statistically similar to normal sessions.

Overall, these visualizations show that while K-Means can successfully identify dense and homogeneous attack patterns such as DoS, it struggles to separate attack types that are sparse, subtle, or overlapping with normal traffic.

\subsection{DB-Scan}
Finally, we used DB-Scan, a clustering algorithm designed to detect anomalous patterns that may represent anomalies.

\paragraph{Choosing \emph{min\_points} and $\epsilon$} To choose the most appropriate value for \emph{min\_points}, we used two different approaches. First, we tried to estimate it running K-Means with different numbers of clusters, and taking the smallest cluster containing only \texttt{normal} points. With 8 clusters, there is a pure cluster formed by 10 \texttt{normal} points. The same results can be obtained using 9 clusters. Assuming \emph{min\_points} equal to 10, and using the elbow rule on the k-distance graph, we selected $\epsilon=0.25$.  However, this approach might be somehow misleading, since we are relying on the labels for an unsupervised algorithm. Moreover, this number of \emph{min\_points} is very small, hence the algorithm may tend to create clusters whenever some points are close to each other, even though they are few, anomalous and very far from the others. Hence, we tried a different approach. First, we computed the cosine distance of each point from its first 3000 neighbors. Then, we averaged these distances for all points in the training set, trying both to include all samples and to exclude points with values quite distant from the average, but these choice did not affect the result. We noticed that the average distance of a point from its $3000^{th}$ neighbor is around \num{0.5}, which means that the space is very cohesive, considering that the cosine distance can be at most 1. Hence, we chose as value of \emph{min\_points} 1600: a point has average distance of \num{0.32} from its $1600^{th}$ neighbor, and  the distance tends to increase more abruptly close to this number, playing the role of an elbow. This value of \emph{min\_points} seemed more reasonable in respect to our goal, since we wish to flag all attack points as anomalous, hence we want to create big clusters of normal points only. As a consequence, we ran DB-Scan first using $min\_points = 10$ and $\epsilon = 0.25$, and then $min\_points = 1600$, $\epsilon = 0.32$, and $metric = cosine$, which is the distance used to emphasize directional similarity in the high-dimensional feature space and reduce the impact of magnitude differences caused by traffic volume.

\begin{table}
\footnotesize
\centering
\caption{Clustering performed by DB-Scan algorithm.}
\label{tab:task4_dbscan_clusters}
\begin{tabular}{ccccc}
\toprule
{\textbf{Attack Label}} & {\textbf{Noise Cluster}} & {\textbf{Cluster 0}} & {\textbf{Cluster 1}} & {\textbf{Cluster 2}} \\
\midrule
Normal 	& 2736 	& 67 	& 9389 	& 1256 	\\
DoS 	& 586 	& 1657 	& 193 	& 477 	\\
Probe 	& 2073 	& 89 	& 99 	& 28 	\\
R2L 	& 69 	& 1 	& 111 	& 0 	\\ \midrule
Total 	& 5464 	& 1814 	& 9792 	& 1761	\\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Clusters analysis} With a small value of \emph{min\_points} the algorithm generates 100 clusters (plus the noise one) and detects 7031 points as noise (\SI{37.34}{\percent}), but 5176 (\SI{74}{\percent}) are \texttt{normal} samples (\SI{14}{\percent} \texttt{probe}, \SI{11}{\percent} \texttt{dos}, \SI{1}{\percent} \texttt{r2l}). Given these scarce results, from now on we will focus on the second clustering performed ($min\_points = 1600$, $\epsilon = 0.32$). \Cref{tab:task4_dbscan_clusters} summarizes the resulting clustering. DB-Scan identified 5464 noise points (\SI{29.0}{\percent}), which closely matches the true proportion of anomalous traffic in the dataset.  However, the noise cluster is composed half of \texttt{normal} samples and half of attack samples, especially \texttt{probe} ones. Only \SI{20}{\percent} of \texttt{dos} points have been flagged as noise, against \SI{38}{\percent} of \texttt{r2l} and \SI{90}{\percent} of \texttt{probe} ones. It means that \texttt{probe} points tend to be sparse and isolated in the feature space, while \texttt{dos} ones form dense and cohesive clusters due to their high traffic volume and repetitive patterns. Cluster 0 is mainly formed by \texttt{dos} samples, while cluster 1 is composed for the \SI{96}{\percent} of \texttt{normal} points. Cluster 2, instead, is composed for \SI{71}{\percent} of \texttt{normal} samples, and for the \SI{27}{\percent} of \texttt{dos} ones, indicating that DB-Scan struggles when anomalous behavior itself becomes dense and overlaps with benign traffic. Overall, we deduce that DB-Scan is effective at finding ``sparse'' anomalies like \texttt{probe} ones, while it struggles when attacks generate dense traffic patterns (like the \texttt{dos} samples assigned to cluster 1 by K-Means in \cref{sec:kmeans}), even if choosing a high value for \emph{min\_points}.

