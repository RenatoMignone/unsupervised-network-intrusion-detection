\section{Deep Anomaly Detection and Data Representation}\label{sec:task3}
\subsection{Autoencoder Architecture and Training}

We implemented an \textit{Improved Autoencoder} designed to handle the mixed nature of our dataset (numerical and categorical features).

\subsubsection{Model Architecture}
The architecture consists of a symmetric Encoder-Decoder structure:
\begin{itemize}
    \item \textbf{Encoder:} It consists of linear layers expanding dimensions from the input size to 128, and then reducing them to 64, and finally to a bottleneck of size 16. Each intermediate layer is followed by Batch Normalization, ReLU activation, and Dropout (0.2) to prevent overfitting.
    \item \textbf{Decoder:} It mirrors the encoder, expanding dimensions from 16 to 64, 128, and finally back to the original input dimension.
\end{itemize}

Since the dataset contains both numerical and categorical features (one-hot encoded), we employed a composite loss function. This ensures that the model learns to reconstruct continuous values accurately (via Mean Squared Error) while correctly classifying the categorical attributes (via Cross-Entropy).

The Autoencoder was trained in a semi-supervised manner, using only normal data from the training set. The objective is to learn the manifold of normal traffic patterns. Consequently, the model is expected to reconstruct normal samples well (yielding low error) while failing to accurately reconstruct anomalies (resulting in high error). In particular, we split the normal data from the original training dataset into training and validation sets, with a proportion of 8:2.

\begin{figure}
	\centering
	\subfloat[][{Loss curve on training set.}\label{fig:task3_train_curve}]
	{\includegraphics[width=.47\linewidth]{img/Task3/task3_autoencoder_all_train_loss.png}} \quad
	\subfloat[][{Loss curve on validation set.}\label{fig:task3_val_curve}]
	{\includegraphics[width=.47\linewidth]{img/Task3/task3_autoencoder_all_val_loss.png}} \\
  	\caption{Training and Validation Loss curves of the Autoencoder during the learning rate grid search ($lr \in \{0.005, 0.001, 0.0005, 0.0001\}$).}{}\label{fig:task3_loss_curves}
\end{figure}

\subsection{Hyperparameter Tuning and Model Selection}
We performed a grid search to select the optimal learning rate, testing values in \{\num{0.005}, \num{0.001}, \num{0.0005}, \num{0.0001}\}. The models were trained for up to 100 epochs: the trends of training and validation losses are shown in \cref{fig:task3_loss_curves}. The best model was selected based on the minimum validation loss observed. The learning rate of $0.0005$ provided the most stable convergence and the lowest validation loss, avoiding the instability seen with higher rates ($0.005$) or the slow convergence obtained with lower rates (e.g., $0.0001$). Even though the curves of leaning rates $0.0005$ and $0.001$ are quite similar, we preferred the first one for its stability. For the final model, we chose 50 as number of epochs, since the validation loss does not improve after that epoch, and in this way we reduce the risk of overfitting.

\subsection{Reconstruction Error Analysis}
\paragraph{Threshold Selection}
After training, we analyzed the distribution of reconstruction errors across the validation set, shown in \cref{fig:task3_ecdf_val}. We selected the threshold based on the elbow point: the chosen value is 0.3. After that value, the reconstruction error increases more abruptly. Specifically, $\approx\SI{90}{\percent}$ of the samples has a reconstruction error lower than the identified threshold.

\paragraph{Reconstruction errors for each set}
\Cref{fig:task3_ecdf_all} presents the Empirical Cumulative Distribution Function (ECDF) of the reconstruction errors for the validation, full training and test sets.
\begin{itemize}
    \item \textbf{Validation Set (Blue):} Shows the lowest errors, as it consists only of normal data.
    \item \textbf{Training Set (Purple):} The curve is shifted to the left with respect to the previous one. The error exceeds the threshold after $\approx$~\SI{65}{\percent} of samples, which is quite consistent with the ratio of anomalies present in the dataset (\SI{29}{\percent}). This is due to the presence of anomalies in the full training set (which were excluded during AE training), which the model fails to reconstruct.
    \item \textbf{Test Set (Orange):} Shows the highest errors. The error exceeds the threshold after $\approx$~\SI{25}{\percent} of samples, which reasonably aligns with the amount of anomalies in the test set (\SI{63}{\percent}).
\end{itemize}

\begin{figure}
	\centering
	\subfloat[][{ECDF curve of the reconstruction errors on the validation set.}\label{fig:task3_ecdf_val}]
	{\includegraphics[width=.47\linewidth]{img/Task3/task3_ecdf_reconstruction_error_validation.png}} \quad
	\subfloat[][{ECDF curve reconstruction errors on the validation, full training and test sets.}\label{fig:task3_ecdf_all}]
	{\includegraphics[width=.47\linewidth]{img/Task3/task3_ecdf_reconstruction_error_combined.png}} \\
  	\caption{ECDF curves of reconstruction errors for the different sets. The red line indicates the identified threshold.}{}\label{fig:task3_ecdf}
\end{figure}
\paragraph{Model performance} Finally, we used the identified threshold to perform the classification: if a sample has a reconstruction error higher than 0.3, it is classified as an anomaly. \Cref{tab:task3_ae} summarizes the performance across the entire training, validation and test sets. The model is quite good at recognizing the anomalies on the training set, as demonstrated by the high precision and recall on both classes. The validation set is only composed of normal samples, hence the metrics related to the anomaly class are zero. Nevertheless, only 259 samples out of 2690 (\SI{10}{\percent}) are incorrectly classified as anomalies, mirroring the reasoning performed when selecting the threshold. Finally, the model demonstrates to be quite good at generalizing on the test set, with an accuracy of \num{0.78}. Precision is around \num{0.80} for both classes, while recall is higher on the anomaly class (0.92). The model, in fact, tends to classify \SI{45}{\percent} of normal samples as anomalies, probably because they exhibit different characteristics with respect to those in the training set, so the model fails in properly reconstructing them.

\begin{table}
    \centering
    \small
    \caption{Performance on full training, validation and test sets obtained with the Autoencoder.}
    \label{tab:task3_ae}
    \begin{tabular}{lcccccccccccc}
        \toprule
        \textbf{Class} & \multicolumn{3}{c}{\textbf{Precision}} & \multicolumn{3}{c}{\textbf{Recall}} & \multicolumn{3}{c}{\textbf{F1-Score}} & \multicolumn{3}{c}{\textbf{Accuracy}}\\
        \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13}
         & \textbf{Train} & \textbf{Val} & \textbf{Test} & \textbf{Train} & \textbf{Val} & \textbf{Test} & \textbf{Train} & \textbf{Val} & \textbf{Test} & \textbf{Train} & \textbf{Val} & \textbf{Test} \\
        \midrule
        Normal & 0.97 & 1.00 & 0.79 & 0.89 & 0.89 & 0.55 & 0.93 & 0.94 & 0.65 & \multirow{2}{2em}{0.90} & \multirow{2}{2em}{0.89} & \multirow{2}{2em}{0.78}\\
        Anomaly & 0.78 & 0.00 & 0.78 & 0.93 & 0.00 & 0.92 & 0.85 & 0.00 & 0.84 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Latent Space Anomaly Detection (AE + OC-SVM)}

We extracted the 16-dimensional bottleneck representation of the training set with only normal data and trained a OC-SVM on these latent features. As estimation of the $\nu$ parameter, we used \num{0.1}: in the previous section, in fact, we observed that \SI{10}{\percent} of normal data tends to have a high reconstruction error, so they can be considered as anomalous by the model, that, otherwise, would create too loose boundaries. Then, we evaluated the model on both full training and test sets. The results are presented in \cref{tab:task3_ae_ocsvm_classification}.
\begin{table}
    \centering
    \small
    \caption{Classification Report for OC-SVM trained on Autoencoder Bottleneck Features.}
    \label{tab:task3_ae_ocsvm_classification}
    \begin{tabular}{lcccccccccccc}
        \toprule
        \textbf{Class} & \multicolumn{2}{c}{\textbf{Precision}} & \multicolumn{2}{c}{\textbf{Recall}} & \multicolumn{2}{c}{\textbf{F1-Score}} & \multicolumn{2}{c}{\textbf{Accuracy}}\\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
         & \textbf{Train} & \textbf{Test} & \textbf{Train} & \textbf{Test} & \textbf{Train} &  \textbf{Test} & \textbf{Train} &  \textbf{Test} \\
        \midrule
        Normal & 0.85 & 0.63 & 0.90 & 0.72 & 0.87 & 0.67 &  \multirow{2}{2em}{0.82} & \multirow{2}{2em}{0.74} \\
        Anomaly & 0.71 & 0.82 & 0.61 & 0.75 & 0.65 & 0.79 \\
        \bottomrule
    \end{tabular}
\end{table}
\paragraph{Performance Analysis}
The AE+OC-SVM approach achieves an overall accuracy of \SI{82}{\percent} on the training set and \SI{74}{\percent} on the test set. Notably, on the training set, the model exhibits high recall for normal traffic (\SI{90}{\percent}), but a significantly lower value for anomalies (\SI{61}{\percent}). On the test set, instead, these two values are more similar ($0.72$ for normal data and $0.75$ for anomalies). However, the model tends to classify anomalies as normal data, especially on the test set, as shown by the relatively low precision (\num{0.63}). Hence, the model tends to miss anomalies, while generating fewer false alarms. Given these results, we also experimented with a smaller value of $\nu$ (\num{0.01}) that better matches the percentage of noise we expect in normal data, but we got lower performance on the full training set (accuracy: \num{0.76}, macro F1 score: \num{0.57}). It means that the encoder layer is probably emphasizing the intrinsic differences among normal data, conflating some with anomalies.

\paragraph{Model Comparison}
\Cref{tab:task3_model_comparison} summarizes the accuracy and the macro F1 score of the original OC-SVM model (trained with normal data only and $\nu = 0.001$), the Autoencoder with reconstruction error (AE) and the AE+OC-SVM model ($\nu = 0.1$). The first two models obtain similar performance, even though AE performs slightly better on the test set. The AE+OC-SVM  model, instead, performs worse on both the training and test set, with lower accuracy and macro F1 scores than the AE one. The Autoencoder acts as a non-linear feature extractor, compressing the essential information of ``normality'' into the bottleneck while discarding noise. However, it fails reconstructing the anomalous samples: while it is an advantage when using the reconstruction error as discriminating factor, it is not perfectly representative when using OC-SVM.

\begin{table}
\small
\centering
\caption{Performance comparison on training and test set of OC-SVM, Autoencoder with reconstruction error (AE), OC-SVM trained with the Autoencoder's embeddings (AE+OC-SVM) and OC-SVM trained with PCA components (PCA+OC-SVM).}
\label{tab:task3_model_comparison}
\begin{tabular}{lcccc}
\toprule
{\textbf{Model}} & {\textbf{Accuracy$_{train}$}} & {\textbf{Accuracy$_{test}$}} & {\textbf{Macro F1 score$_{train}$}} & {\textbf{Macro F1 score$_{test}$}} \\
\midrule
Normal-Only OC-SVM & 0.93 & 0.74 	& 0.90 	& 0.73 	\\
AE & 0.90 & 0.78 	& 0.89 	& 0.75 	\\
AE + OC-SVM & 0.82 & 0.74 	& 0.76 	& 0.73 	\\
PCA + OC-SVM & 0.91 & 0.74 	& 0.88 	& 0.74 	\\
\bottomrule
\end{tabular}
\end{table}

\subsection{PCA-based Anomaly Detection}

Finally, we explored a linear dimensionality reduction technique: Principal Component Analysis (PCA). We analyzed the cumulative explained variance to determine the optimal number of components. Specifically, we found the elbow point, then selected 20 components to capture \SI{95}{\percent} of the variance. Then, we trained an OC-SVM on this reduced space using normal data only, setting $\nu$ as the proportion of noise we expect in normal data as in \cref{sec:task2} (\num{0.001}).

\begin{table}
    \centering
    \small
    \caption{Classification Report for OC-SVM trained on PCA-reduced features.}
    \label{tab:task3_pca_classification}
    \begin{tabular}{lcccccccccccc}
        \toprule
        \textbf{Class} & \multicolumn{2}{c}{\textbf{Precision}} & \multicolumn{2}{c}{\textbf{Recall}} & \multicolumn{2}{c}{\textbf{F1-Score}} & \multicolumn{2}{c}{\textbf{Accuracy}}\\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
         & \textbf{Train} & \textbf{Test} & \textbf{Train} & \textbf{Test} & \textbf{Train} &  \textbf{Test} & \textbf{Train} &  \textbf{Test} \\
        \midrule
        Normal & 0.89 & 0.61 & 1.00 & 0.81 & 0.94 & 0.70 &  \multirow{2}{2em}{0.91} & \multirow{2}{2em}{0.74} \\
        Anomaly & 0.98 & 0.86 & 0.70 & 0.70 & 0.82 & 0.77 \\
        \bottomrule
    \end{tabular}
\end{table}

\paragraph{Performance Analysis} \Cref{tab:task3_pca_classification} shows the results for the PCA-based model. On the training set, the model performs better on the normal class, as shown by the high F1 score (\num{0.94}). On the test set, instead, it achieves higher metrics on the anomalous class (\num{0.77} of F1 score). Overall, accuracy is fairly good, both on the training set (\num{0.91}) and on the test set (\num{0.74}).

\paragraph{Model Comparison}
\Cref{tab:task3_model_comparison} summarizes the accuracy and the macro F1 score of the original OC-SVM model (trained with normal data only and $\nu = 0.001$), the Autoencoder with reconstruction error (AE), the AE+OC-SVM model ($\nu = 0.1$) and the PCA+OC-SVM model ($\nu = 0.001$).  The last model outperforms the AE+OC-SVM model, especially on the training set. It performs similarly to the original OC-SVM model and to the Autoencoder with reconstruction error. In particular, the PCA+OC-SVM achieves lower precision but higher recall on the benign class than the AE, while for the anomalies it is the vice versa. Even though PCA is a linear method, it performs quite good, indicating that the main relationships between features are linear.