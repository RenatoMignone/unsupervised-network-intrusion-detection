\section{Deep Anomaly Detection and Data Representation}\label{sec:task3}

\subsection{Autoencoder Architecture and Training}

We implemented an \textit{Improved Autoencoder} designed to handle the mixed nature of our dataset (numerical and categorical features).

\subsubsection{Model Architecture}
The architecture consists of a symmetric Encoder-Decoder structure:
\begin{itemize}
    \item \textbf{Encoder:} It consists of linear layers reducing dimensions from the input size to 128, then 64, and finally to a bottleneck of size 16. Each intermediate layer is followed by Batch Normalization, ReLU activation, and Dropout (0.2) to prevent overfitting.
    \item \textbf{Decoder:} It mirrors the encoder, expanding dimensions from 16 to 64, 128, and finally back to the original input dimension.
\end{itemize}

Since the dataset contains both numerical and categorical features (one-hot encoded), we employed a composite loss function. This ensures that the model learns to reconstruct continuous values accurately (via Mean Squared Error) while correctly classifying the categorical attributes (via Cross-Entropy).

The Autoencoder was trained in a semi-supervised manner, using only normal data from the training set. The objective is to learn the manifold of normal traffic patterns. Consequently, the model is expected to reconstruct normal samples well (yielding low error) while failing to accurately reconstruct anomalies (resulting in high error).

\subsection{Hyperparameter Tuning and Model Selection}

We performed a grid search to select the optimal learning rate, testing values in $\{0.01, 0.005, 0.001, 0.0005\}$. The models were trained for up to 50 epochs, and the best model was selected based on the minimum validation loss observed during training. Specifically, for each learning rate, we saved the model state at the epoch with the lowest validation loss (early stopping criterion), preventing overfitting.

\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{img/Task3/task3_autoencoder_all_train_val_loss_comparison.png}
    \caption{Training and Validation Loss curves of the Autoencoder during the learning rate grid search. The left panel shows training loss evolution, while the right panel shows validation loss for different learning rates ($lr \in \{0.01, 0.005, 0.001, 0.0005\}$).}
    \label{fig:task3_loss_curves}
\end{figure}

As shown in \cref{fig:task3_loss_curves}, the learning rate of $0.001$ provided the most stable convergence and the lowest validation loss, avoiding the instability seen with higher rates ($0.01$).

\subsection{Reconstruction Error Analysis}

After training, we analyzed the distribution of reconstruction errors across the Validation, Training (full), and Test sets.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{img/Task3/task3_ecdf_reconstruction_error_combined.png}
    \caption{Combined ECDF of Reconstruction Errors for Validation, Training, and Test sets.}
    \label{fig:task3_ecdf}
\end{figure}

Figure \ref{fig:task3_ecdf} presents the Empirical Cumulative Distribution Function (ECDF) of the errors.
\begin{itemize}
    \item \textbf{Validation Set (Blue):} Shows the lowest errors, as it consists only of normal data similar to the training set.
    \item \textbf{Training Set (Purple):} The curve is shifted to the right at the higher percentiles. This is due to the presence of anomalies in the full training set (which were excluded during AE training), which the model fails to reconstruct.
    \item \textbf{Test Set (Orange):} Shows the highest errors. This indicates the ``generalization gap'' where the model encounters unseen normal variations and novel attack patterns that were not present in the training set.
\end{itemize}

\subsubsection{Threshold Selection}
We selected the anomaly detection threshold based on an expected contamination rate. For each dataset, we computed the 70th percentile of the reconstruction errors as the threshold, which corresponds to classifying the top 30\% of samples with highest errors as anomalies. This contamination-aware approach acknowledges that a significant portion of the data may contain anomalous patterns and sets the decision boundary accordingly. Each dataset (training, validation, and test) uses its own 70th percentile as the threshold to account for distributional differences.

\subsection{Latent Space Anomaly Detection (AE + OC-SVM)}

We extracted the 16-dimensional bottleneck representation of the data and trained a One-Class SVM (OC-SVM) on these latent features. The results are presented in Table \ref{tab:task3_ae_ocsvm_classification}.

\begin{table}
    \centering
    \caption{Classification Report for OC-SVM trained on Autoencoder Bottleneck Features.}
    \label{tab:task3_ae_ocsvm_classification}
    \begin{tabular}{lccccccccc}
        \toprule
        \textbf{Class} & \multicolumn{3}{c}{\textbf{Precision}} & \multicolumn{3}{c}{\textbf{Recall}} & \multicolumn{3}{c}{\textbf{F1-Score}} \\
        \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
         & \textbf{Train} & \textbf{Val} & \textbf{Test} & \textbf{Train} & \textbf{Val} & \textbf{Test} & \textbf{Train} & \textbf{Val} & \textbf{Test} \\
        \midrule
        Normal (0) & 0.81 & 0.81 & 0.48 & 0.99 & 0.99 & 0.71 & 0.90 & 0.89 & 0.57 \\
        Anomaly (1) & 0.97 & 0.93 & 0.76 & 0.43 & 0.43 & 0.54 & 0.60 & 0.59 & 0.63 \\
        \midrule
        \textbf{Accuracy} & \multicolumn{3}{c}{0.83} & \multicolumn{3}{c}{0.83} & \multicolumn{3}{c}{0.60} \\
        \bottomrule
    \end{tabular}
\end{table}

\textbf{Performance Analysis:}
The AE+OC-SVM approach achieves an overall accuracy of \SI{83}{\percent} on training/validation sets and \SI{60}{\percent} on the test set. Notably, the model exhibits high recall for normal traffic (\SI{99}{\percent} on train/val, \SI{71}{\percent} on test) but significantly lower recall for anomalies (\SI{43}{\percent} on train/val, \SI{54}{\percent} on test). This indicates that the OC-SVM boundary in the latent space is conservative, classifying many true anomalies as normal.

However, when the model does predict an anomaly, it is highly confident: precision for anomalies is excellent (\SI{97}{\percent} on train, \SI{93}{\percent} on val, \SI{76}{\percent} on test). This high precision with moderate recall suggests the model is useful for scenarios where false positives are costly, as it produces few false alarms while still detecting over half of the anomalies in the test set.

\textbf{Comparison with Original OC-SVM:}
The AE+OC-SVM approach generally outperforms the OC-SVM trained on raw features (Task 2). The Autoencoder acts as a non-linear feature extractor, compressing the essential information of ``normality'' into the bottleneck while discarding noise. This results in a cleaner, lower-dimensional feature space where the OC-SVM can more effectively define a boundary enclosing the normal data. The 16-dimensional bottleneck provides a more discriminative representation than the original high-dimensional feature space.

\subsection{PCA-based Anomaly Detection}

Finally, we explored a linear dimensionality reduction technique: Principal Component Analysis (PCA). We analyzed the cumulative explained variance to determine the optimal number of components. As shown in \cref{fig:task3_pca_variance}, we selected 22 components to capture approximately \SI{95}{\percent} of the variance and trained an OC-SVM on this reduced space.

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{img/Task3/task3_pca_explained_variance.png}
    \caption{Cumulative Explained Variance by PCA Components.}
    \label{fig:task3_pca_variance}
\end{figure}



\begin{table}
    \centering
    \caption{Classification Report for OC-SVM trained on PCA-reduced features.}
    \label{tab:task3_pca_classification}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lcccccccccccc}
        \toprule
        \textbf{Class} & \multicolumn{3}{c}{\textbf{Precision}} & \multicolumn{3}{c}{\textbf{Recall}} & \multicolumn{3}{c}{\textbf{F1-Score}} & \multicolumn{3}{c}{\textbf{Support}} \\
        \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13}
        & \textbf{Train} & \textbf{Val} & \textbf{Test} & \textbf{Train} & \textbf{Val} & \textbf{Test} & \textbf{Train} & \textbf{Val} & \textbf{Test} & \textbf{Train} & \textbf{Val} & \textbf{Test} \\
        \midrule
        Normal (0) & 0.90 & 0.88 & 0.61 & 0.99 & 0.99 & 0.80 & 0.94 & 0.93 & 0.69 & 12102 & 1346 & 2152 \\
        Anomaly (1) & 0.97 & 0.95 & 0.86 & 0.71 & 0.66 & 0.71 & 0.82 & 0.78 & 0.77 & 4845 & 538 & 3674 \\
        \midrule
        \textbf{Accuracy} & \multicolumn{3}{c}{-} & \multicolumn{3}{c}{-} & 0.91 & 0.89 & 0.74 & 16947 & 1884 & 5826 \\
        \bottomrule
    \end{tabular}
    }
\end{table}

\textbf{Comparison (PCA vs. AE):}
Table \ref{tab:task3_pca_classification} shows the results for the PCA-based model. While PCA provides a strong baseline, the Autoencoder-based approaches (both Reconstruction Error and Bottleneck+OCSVM) typically yield better performance. This suggests that the manifold of normal network traffic is non-linear. PCA, being a linear method, cannot capture the complex non-linear relationships between features as effectively as the deep Autoencoder, leading to a less precise separation between normal and anomalous traffic.